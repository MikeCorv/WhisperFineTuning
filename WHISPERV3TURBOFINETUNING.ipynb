{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPESghyHOnuc4kGhBDzcuNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeCorv/WhisperFineTuning/blob/main/WHISPERV3TURBOFINETUNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets librosa soundfile accelerate\n",
        "!pip install -q bitsandbytes peft"
      ],
      "metadata": {
        "id": "mk9417u2bUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchcodec"
      ],
      "metadata": {
        "id": "xUvffJuR-Lx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets[audio]"
      ],
      "metadata": {
        "id": "_4NQw9kM-mrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzNyUyvFaHAx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ybajjIMebJDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if Colab GPU is detected\n",
        "print(\"3. Checking Hardware...\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"SUCCESS: GPU Detected -> {gpu_name}\")\n",
        "    # Checking VRAM\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"VRAM available: {vram:.2f} GB\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: No GPU detected. Go to Runtime -> Change runtime type -> T4 GPU.\")"
      ],
      "metadata": {
        "id": "DYk1B5VNbMha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"openai/whisper-large-v3-turbo\""
      ],
      "metadata": {
        "id": "ZrRZXn-Ucgkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4bit quantization since I don't have Colab Pro (buying it soon lol)\n",
        "print(f\"Defining Quantization Config for {MODEL_ID}...\")\n",
        "# ->4bit format (Cut the model size by 4x, again: I don't have Colab Pro)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # Best precision for 4-bit/more details here: https://www.emergentmind.com/topics/4-bit-normalfloat-nf4-quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    #From QLORA Paper(https://arxiv.org/pdf/2305.14314): QLORA has one low-precision storage data type, in our case usually 4-bit, and one computation data type that is usually BFloat16.\n",
        "    #In practice, this means whenever a QLORA weight tensor is used, we dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\n",
        "    #for CausalLM models, the last lm_head is kept in its original dtype.\"\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JQq17jqybiDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading and Loading the Model (this handles the weights)...\")\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",    # Automatically put it on the GPU\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "i8v5Zreycj0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the \"compressed\" model\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "4ash1j5Kc9r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"3. Loading the Processor (The 'Translator')...\")\n",
        "# The processor handles: Audio -> Spectrogram AND Text -> Tokens\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, language=\"italian\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "K1WQDfzQdAad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RESET MODEL STATE ---\n",
        "print(\"Checking model state...\")\n",
        "\n",
        "# Check if the model is currently wrapped in LoRA\n",
        "if hasattr(model, \"unload\"):\n",
        "    # This strips off the LoRA layers and returns the raw Base Model\n",
        "    model = model.unload()\n",
        "    print(\"✅ SUCCESS: Old adapters unloaded. Model is back to clean base state.\")\n",
        "else:\n",
        "    print(\"ℹ️ NOTE: Model was already clean (no adapters found).\")"
      ],
      "metadata": {
        "id": "jjCfId6hQxB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapters\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "FzPVSzXxdKFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining LoRA Config...\")\n",
        "#Removed \"task_type\" argument after checking this: https://github.com/huggingface/peft/issues/1988\n",
        "lora_config = LoraConfig(\n",
        "    r=32,               # The \"Rank\": How complex the new brain paths are\n",
        "    lora_alpha=64,      # Scaling factor (usually 2x the Rank)\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Attach only to Attention layers\n",
        "    lora_dropout=0.05,  # Randomly turn off 5% of neurons to prevent memorization\n",
        "    bias=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y_HdRNmviXt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Injecting Adapters into the Model...\")\n",
        "# This wraps the base model with the new LoRA layers\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "3d_-SnQfiYuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- PARAMETER CHECK ---\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "VYKcBNDMicov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reloading clean data from my Drive\n",
        "\n",
        "from datasets import load_from_disk\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/fleurs_it_processed\"\n",
        "dataset = load_from_disk(DATA_PATH)\n",
        "print(f\"Dataset Loaded! Train size: {len(dataset['train'])} | Test size: {len(dataset['test'])}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2x9NvNs4iedp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # A. Separate Audio (Inputs) and Text (Labels)\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # B. Pad Audio to the longest in the batch\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # C. Pad Text to the longest sentence\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # D. Mask Padding\n",
        "        # We replace the \"padding zeros\" with -100.\n",
        "        # This tells the model: \"Don't try to predict these empty spots, they don't count.\"\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # E. Remove \"Start of Sentence\" token if present (Whisper adds it automatically)\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ],
      "metadata": {
        "id": "U8jSJeYtjOgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "print(\"SUCCESS: Data is loaded and the Collator is ready.\")"
      ],
      "metadata": {
        "id": "PgxKZCFgjS_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INSPECTION BLOCK ---\n",
        "import numpy as np\n",
        "\n",
        "raw_example = dataset[\"train\"][0]\n",
        "print(f\"Keys available: {list(raw_example.keys())}\")\n",
        "print(f\"Audio content:   '{raw_example['audio']}'\")\n",
        "print(f\"Text content:   '{raw_example['sentence']}'\")\n",
        "print(f\"Duration:   '{raw_example['duration']}'\")"
      ],
      "metadata": {
        "id": "RD3WzxsulrO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Columns currently in dataset: {dataset['train'].column_names}\")"
      ],
      "metadata": {
        "id": "ms3d2dyeBPqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_test = dataset[\"train\"][0][\"audio\"]\n",
        "print(f\"Audio content:   '{audio_test}'\")"
      ],
      "metadata": {
        "id": "n_cicsgcDzU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_extraction_test = processor.feature_extractor(audio_test[\"array\"], sampling_rate=audio_test[\"sampling_rate\"])\n",
        "print(f\"Audio content:   '{audio_extraction_test}'\")"
      ],
      "metadata": {
        "id": "KM6vLtwhD6eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(audio_extraction_test.input_features[0])"
      ],
      "metadata": {
        "id": "MaWH3SWjGgN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_test = processor.tokenizer(dataset['train'][0][\"sentence\"])\n",
        "print(tokenizer_test)"
      ],
      "metadata": {
        "id": "4C60eeV9G2wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    #Extracting the Audio Object\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    #Audio -> Spectrogram\n",
        "    batch[\"input_features\"] = processor.feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_features[0]\n",
        "\n",
        "    # 3.Text -> Token IDs\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(\n",
        "prepare_dataset,\n",
        "remove_columns=dataset[\"train\"].column_names, # Remove 'audio', 'sentence', etc.\n",
        "num_proc=1, # Keep it safe on RAM\n",
        "desc=\"Feature Extraction\"\n",
        ")\n",
        "\n",
        "print(\"SUCCESS: Dataset is now in 'Model Format' (input_features, labels).\")"
      ],
      "metadata": {
        "id": "RWvETjwPARiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Columns currently in dataset: {dataset['train'].column_names}\")"
      ],
      "metadata": {
        "id": "7dPBgIVtBuEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for evaluating the model, we'll use Just In Time WER\n",
        "!pip install -q evaluate jiwer"
      ],
      "metadata": {
        "id": "hVF74x6_jeFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "GykLQg1_j24G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace the \"ignore\" index (-100) with the pad token so we can decode\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode the model's guess and the correct answer back to text\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate the error rate\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "gGYIcW1CkCFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-finetuned-lora\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-3,\n",
        "    max_steps=500,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    save_steps=250,\n",
        "    eval_steps=250,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"] # Explicitly tell it where the answers are\n",
        ")"
      ],
      "metadata": {
        "id": "iqTlQH8hkEcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "up_K4VmMkgwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_Re4WpOGkhsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADAPTER_PATH = \"/content/drive/MyDrive/whisper-large-v3-turbo-italian-lora\"\n",
        "\n",
        "print(f\"Saving adapters to {ADAPTER_PATH}...\")\n",
        "\n",
        "model.save_pretrained(ADAPTER_PATH)\n",
        "processor.save_pretrained(ADAPTER_PATH)\n"
      ],
      "metadata": {
        "id": "j2pQfD6sMauy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}